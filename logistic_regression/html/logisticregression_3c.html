
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>logisticregression</title><meta name="generator" content="MATLAB 8.1"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2014-02-26"><meta name="DC.source" content="logisticregression.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Logistic Regression</a></li><li><a href="#2">Import training data</a></li><li><a href="#3">Training by Gradient Descent</a></li><li><a href="#4">Testing</a></li><li><a href="#5">Results</a></li></ul></div><h2>Logistic Regression<a name="1"></a></h2><pre class="codeinput">clc;
clear;
close <span class="string">all</span>;
<span class="comment">% Type of target: options=1 for binary target, options=2 for multi-class</span>
options = 2;
d = 3;
max_iter = 450;
eta = 0.5;
<span class="comment">% This is for 3 class regression</span>
c = 3;
</pre><h2>Import training data<a name="2"></a></h2><p>They both include four columns. The first column coded the target variable of ?apply to graduate school?, unlikely (0), or  likely (1) in the binary data,  and  unlikely (0), somewhat likely (1), or very likely (2) in the 3-class data. The other three columns are three variables as follows: 1.parent, which is a 0/1 variable indicating whether at least one parent has a graduate degree; 2.public, which is a 0/1 variable where 1 indicates that the undergraduate institution is a public university and 0 indicates that it is a private university, 3.gpa, which is the student's grade point average.</p><p>In other words, each undergraduate student is described by  x, which is a 3-dim vector.  Can we make a prediction of his/her target t=?</p><pre class="codeinput"><span class="keyword">if</span> options == 1
    train = importdata(<span class="string">'logreg_data_binary.txt'</span>);
<span class="keyword">elseif</span> options == 2
    train = importdata(<span class="string">'logreg_data_3class.txt'</span>);
<span class="keyword">end</span>
<span class="comment">% Normalization</span>
x0 = train(:,2:4);
t = train(:,1);
n = size(x0,1);
x = (x0-repmat(mean(x0),n,1))./repmat(std(x0),n,1);
x = [ones(n,1) x];
norm_param = [mean(x0); std(x0)];
</pre><h2>Training by Gradient Descent<a name="3"></a></h2><pre class="codeinput"><span class="keyword">if</span> options == 1
    W = zeros(d+1,1);
    W_threshold = repmat(0.0001, d+1, 1);
<span class="keyword">elseif</span> options == 2
    wc0 = zeros(d+1,1);  <span class="comment">% [w0 w1 w2 w3]</span>
    wc1 = zeros(d+1,1);
    wc2 = zeros(d+1,1);
    WWW = [wc0 wc1 wc2];
    WWW_threshold = repmat(0.0001, d+1, 3);
    y = zeros(n,c);
    <span class="keyword">for</span> i=1:n
        y(i,t(i)+1) = 1;
    <span class="keyword">end</span>
<span class="keyword">end</span>


<span class="keyword">for</span> iter=1:max_iter
    <span class="keyword">if</span> options == 1
        <span class="comment">% Train W</span>
        W_prev = W;
        mu = 1./(1+exp(-(W'*x')'));
        grad = ((mu-t)'*x)';
        W = W - eta./n*grad;
        NLL(iter) = -sum(t.*log(mu)+(1-t).*log(1-mu));
        <span class="keyword">if</span> abs(W_prev - W) &lt; W_threshold
            <span class="keyword">break</span>;
        <span class="keyword">end</span>
    <span class="keyword">elseif</span> options == 2
        <span class="comment">% Train WWW</span>
        WWW_prev = WWW;
        mu = exp(WWW'*x')'./repmat(sum(exp(WWW'*x')',2),1,c);
        wc0 = wc0-eta./n*((mu(:,1)-y(:,1))'*x)';
        wc1 = wc1-eta./n*((mu(:,2)-y(:,2))'*x)';
        wc2 = wc2-eta./n*((mu(:,3)-y(:,3))'*x)';
        WWW = [wc0 wc1 wc2];
        NLL(iter) = -sum(sum(y'.*(WWW'*x'))'-log(sum(exp(WWW'*x')',2)));
        <span class="keyword">if</span> abs(WWW_prev - WWW) &lt; WWW_threshold
            <span class="keyword">break</span>;
        <span class="keyword">end</span>
    <span class="keyword">end</span>

<span class="keyword">end</span>
</pre><h2>Testing<a name="4"></a></h2><pre class="codeinput"><span class="keyword">if</span> options == 1
    test = importdata(<span class="string">'test_data_binary.txt'</span>);
<span class="keyword">elseif</span> options == 2
    test = importdata(<span class="string">'test_data_3class.txt'</span>);
<span class="keyword">end</span>
<span class="comment">% Normalization</span>
x0 = test(:,2:4);
t = test(:,1);
n = size(x0,1);
x = (x0-repmat(norm_param(1,:),n,1))./repmat(norm_param(2,:),n,1);
x = [ones(n,1) x];

<span class="keyword">if</span> options == 1
    t_hat = (W'*x')';
    t_pred = zeros(size(t));
    t_idx = find(t_hat&gt;=0.5);
    t_pred(t_idx) = ones(size(t_idx));
<span class="keyword">elseif</span> options == 2
    t_hat = (WWW'*x')';
    [max,t_idx] = max(t_hat,[],2);
    t_pred = t_idx-ones(size(t_idx));
<span class="keyword">end</span>
</pre><h2>Results<a name="5"></a></h2><pre class="codeinput">plot(NLL);
title(<span class="string">'Negative Log-likelihood with iteration'</span>);
xlabel(<span class="string">'Iterations'</span>);
ylabel(<span class="string">'Negative Log-likelihood'</span>);
true_idx = find((t_pred - t)==0);
right_ratio = size(true_idx,1)/n;

<span class="keyword">if</span> options == 1
    fprintf(<span class="string">'\nCoefficient W:\n'</span>);
    W
<span class="keyword">elseif</span> options == 2
    fprintf(<span class="string">'\nCoefficient W:\n'</span>);
    WWW
<span class="keyword">end</span>
fprintf(<span class="string">'\n Ratio of correctly predicted targets:\n'</span>);
right_ratio
</pre><pre class="codeoutput">
Coefficient W:

WWW =

    0.4681   -0.0836   -0.3845
   -0.2806    0.0251    0.2555
    0.0632   -0.3694    0.3062
   -0.2023   -0.2479    0.4501


 Ratio of correctly predicted targets:

right_ratio =

    0.4667

</pre><img vspace="5" hspace="5" src="logisticregression_3c.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2013a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Logistic Regression
%
clc;
clear;
close all;
% Type of target: options=1 for binary target, options=2 for multi-class
options = 2;
d = 3;
max_iter = 450;
eta = 0.5;
% This is for 3 class regression
c = 3;
%% Import training data
% They both include four columns. 
% The first column coded the target variable of ?apply to graduate school?,
% unlikely (0), or  likely (1) in the binary data,  and  unlikely (0), somewhat
% likely (1), or very likely (2) in the 3-class data.
% The other three columns are three variables as follows:
% 1.parent, which is a 0/1 variable indicating whether at least one parent 
% has a graduate degree; 
% 2.public, which is a 0/1 variable where 1 indicates that the undergraduate 
% institution is a public university and 0 indicates that it is a private university, 
% 3.gpa, which is the student's grade point average.  
%  
% In other words, each undergraduate student is described by  x, which is a 
% 3-dim vector.  Can we make a prediction of his/her target t=?

if options == 1
    train = importdata('logreg_data_binary.txt');
elseif options == 2
    train = importdata('logreg_data_3class.txt');
end
% Normalization
x0 = train(:,2:4);
t = train(:,1);
n = size(x0,1);
x = (x0-repmat(mean(x0),n,1))./repmat(std(x0),n,1);
x = [ones(n,1) x];
norm_param = [mean(x0); std(x0)];

%% Training by Gradient Descent
if options == 1
    W = zeros(d+1,1);
    W_threshold = repmat(0.0001, d+1, 1);
elseif options == 2
    wc0 = zeros(d+1,1);  % [w0 w1 w2 w3]
    wc1 = zeros(d+1,1);
    wc2 = zeros(d+1,1);
    WWW = [wc0 wc1 wc2];
    WWW_threshold = repmat(0.0001, d+1, 3);
    y = zeros(n,c);
    for i=1:n
        y(i,t(i)+1) = 1;
    end
end


for iter=1:max_iter
    if options == 1
        % Train W
        W_prev = W;
        mu = 1./(1+exp(-(W'*x')'));
        grad = ((mu-t)'*x)';
        W = W - eta./n*grad;
        NLL(iter) = -sum(t.*log(mu)+(1-t).*log(1-mu));
        if abs(W_prev - W) < W_threshold
            break;
        end
    elseif options == 2
        % Train WWW
        WWW_prev = WWW;
        mu = exp(WWW'*x')'./repmat(sum(exp(WWW'*x')',2),1,c);
        wc0 = wc0-eta./n*((mu(:,1)-y(:,1))'*x)';
        wc1 = wc1-eta./n*((mu(:,2)-y(:,2))'*x)';
        wc2 = wc2-eta./n*((mu(:,3)-y(:,3))'*x)';
        WWW = [wc0 wc1 wc2];
        NLL(iter) = -sum(sum(y'.*(WWW'*x'))'-log(sum(exp(WWW'*x')',2)));
        if abs(WWW_prev - WWW) < WWW_threshold
            break;
        end
    end
    
end

%% Testing
if options == 1
    test = importdata('test_data_binary.txt');
elseif options == 2
    test = importdata('test_data_3class.txt');
end
% Normalization
x0 = test(:,2:4);
t = test(:,1);
n = size(x0,1);
x = (x0-repmat(norm_param(1,:),n,1))./repmat(norm_param(2,:),n,1);
x = [ones(n,1) x];

if options == 1
    t_hat = (W'*x')';
    t_pred = zeros(size(t));
    t_idx = find(t_hat>=0.5);
    t_pred(t_idx) = ones(size(t_idx));    
elseif options == 2
    t_hat = (WWW'*x')';
    [max,t_idx] = max(t_hat,[],2);
    t_pred = t_idx-ones(size(t_idx));
end

%% Results
plot(NLL);
title('Negative Log-likelihood with iteration');
xlabel('Iterations');
ylabel('Negative Log-likelihood');
true_idx = find((t_pred - t)==0);
right_ratio = size(true_idx,1)/n;

if options == 1
    fprintf('\nCoefficient W:\n');
    W
elseif options == 2
    fprintf('\nCoefficient W:\n');
    WWW
end
fprintf('\n Ratio of correctly predicted targets:\n');
right_ratio


##### SOURCE END #####
--></body></html>